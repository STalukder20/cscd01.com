{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45cf8c54-98e8-4df6-add0-681fc7c39b22",
   "metadata": {},
   "source": [
    "## Langchain demo of simple sequential chains\n",
    "\n",
    "In class, Alex showed y'all basic simple sequential chains, we will buid on top of this today.\n",
    "First, let's get all the config and imports stuff in order.\n",
    "\n",
    "1. Notice the the below code block has `import config`, find the `config.py` file (in the same directory) and put your openai API key in here.  If you do not have one yet, create a new account and refer to [here](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key) to find your openai API key. \n",
    "\n",
    "2. Once you have your key, update the key in `config.py`, it should have the format of `sk-XXXXXXXXXXXXXXXXXX` (number of X's is not exact so don't take that literally please).\n",
    "\n",
    "3. Run the block below once the above 2 are done so setup the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa9eccd-7af7-4a2c-90b5-a7540a601d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "from langchain.chains import SequentialChain, SimpleSequentialChain\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "import config\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = config.OPENAI_API_KEY\n",
    "\n",
    "# uncomment below if you want to use huggingface instead, when you run this, you will see an input box below, paste in your huggingface token in there\n",
    "# and wait to see \"........\"\n",
    "# from getpass import getpass\n",
    "# HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb71929-3551-4e08-b992-2e1ef0baed5f",
   "metadata": {},
   "source": [
    "### What do we know?\n",
    "\n",
    "We can build a chain by piping the output from 1 prompt into the input of another.  We saw this in class with poems and analysis.\n",
    "\n",
    "The poem template was as such:\n",
    "```\n",
    "poem_template = \"\"\"\n",
    "You are a Poet. Given the title of a poem, write a haiku for that title.\n",
    "Haiku for: \n",
    "{title}\n",
    "\"\"\"\n",
    "\n",
    "and analysis template as such:\n",
    "```\n",
    "analysis_template = \"\"\"You are a poet critic. Given a haiku, it is your job to write an analysis.\n",
    "Poem analysis for:\n",
    "{haiku}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "And we chained them together like this: `overall_chain = SimpleSequentialChain(chains=[poem_chain, analysis_chain], verbose=True)`\n",
    "\n",
    "Now this is pretty cool, but there is something subtle, see if you can spot it.\n",
    "Hint: it has something to do with the initial input variable: title, where did it go?``\n",
    "```\n",
    "o do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880fa7a2-2454-4e07-9728-9eb7920b622d",
   "metadata": {},
   "source": [
    "#### The problem\n",
    "\n",
    "If you have not figured it out already, our initial `title` variable is missing from the output!\n",
    "In fact, imagine a chain as such\n",
    "\n",
    "foo -> bar -> baz\n",
    "\n",
    "where foo takes input a\n",
    "bar takes input b\n",
    "baz takes input c\n",
    "\n",
    "so it becomes foo(a) -> bar(b = foo(a)) -> baz(c = bar(foo(a)))\n",
    "\n",
    "Since every chain effectively ***consumes*** the input, that input is lost forever (or is it??) from the final output.  If you run the chain discussed in class, all we get is the analysis of the poem, we miss the poem title and the poem itself.\n",
    "\n",
    "We will recreate the situation below with food and restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b185a4a-9080-434c-96a8-7b7d6681269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model with temperature\n",
    "# temperature is a measure of how creative we want the model to be\n",
    "temperature = 0.5\n",
    "model = OpenAI(temperature=temperature)\n",
    "\n",
    "# uncomment below to test you have credits, if not go back up and use huggingface\n",
    "name = model(\"say hi\")\n",
    "\n",
    "# uncomment below to use huggingface\n",
    "# repo_id = \"google/flan-t5-xxl\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "# hmodel = HuggingFaceHub(\n",
    "#     repo_id=repo_id, model_kwargs={\"temperature\": temperature, \"max_length\": 64}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d8a0749-ebe1-4ec6-bff9-866660b1aed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "Taco Tequila Palace.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m \n",
      "\n",
      "-Taco Salad \n",
      "-Beef Fajitas \n",
      "-Chicken Quesadillas \n",
      "-Chimichangas \n",
      "-Taco Platter \n",
      "-Grilled Fish Tacos \n",
      "-Nacho Supreme \n",
      "-Chili con Carne \n",
      "-Enchiladas \n",
      "-Tequila Lime Chicken \n",
      "-Taco Pizza \n",
      "-Taco Rice Bowl \n",
      "-Taco Soup \n",
      "-Tequila Shrimp \n",
      "-Tequila Margarita \n",
      "-Fried Ice Cream\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " \n",
      "\n",
      "-Taco Salad \n",
      "-Beef Fajitas \n",
      "-Chicken Quesadillas \n",
      "-Chimichangas \n",
      "-Taco Platter \n",
      "-Grilled Fish Tacos \n",
      "-Nacho Supreme \n",
      "-Chili con Carne \n",
      "-Enchiladas \n",
      "-Tequila Lime Chicken \n",
      "-Taco Pizza \n",
      "-Taco Rice Bowl \n",
      "-Taco Soup \n",
      "-Tequila Shrimp \n",
      "-Tequila Margarita \n",
      "-Fried Ice Cream\n"
     ]
    }
   ],
   "source": [
    "prompt_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"Suggest a fancy name for a restaurant that serves {cuisine} food.\"\n",
    ")\n",
    "\n",
    "# change to hmodel if you want to use huggingface\n",
    "name_chain = LLMChain(llm=model, prompt=prompt_name, output_key=\"restaurant_name\")\n",
    "\n",
    "prompt_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template = \"Suggest some menu items for {restaurant_name}\"\n",
    ")\n",
    "\n",
    "menu_items_chain = LLMChain(llm=model, prompt=prompt_items, output_key=\"menu_items\")\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[name_chain, menu_items_chain], verbose=True)\n",
    "print(overall_chain.run('Mexican'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a867d0-152a-484d-8f61-d388985fadea",
   "metadata": {},
   "source": [
    "If you run the above, notice the last line is just the menu items without the restaurant name!\n",
    "\n",
    "#### Solution\n",
    "\n",
    "In comes SequentialChains, these are more verbose than SimpleSequentialChains and include more information that we can use in other parts of our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f13d1bb9-0ad7-4153-a38f-472a09ad100d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-vhyx88p93vYYVt9KRoCGdsWp on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-vhyx88p93vYYVt9KRoCGdsWp on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-vhyx88p93vYYVt9KRoCGdsWp on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-vhyx88p93vYYVt9KRoCGdsWp on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-vhyx88p93vYYVt9KRoCGdsWp on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-vhyx88p93vYYVt9KRoCGdsWp on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-vhyx88p93vYYVt9KRoCGdsWp on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-vhyx88p93vYYVt9KRoCGdsWp on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Chinese',\n",
       " 'restaurant_name': '\\n\\nImperial Dragon Palace.',\n",
       " 'menu_items': '\\n\\n- Crispy Aromatic Duck\\n- Kung Pao Chicken\\n- Sweet & Sour Pork\\n- Hot & Sour Soup\\n- Egg Fried Rice\\n- Szechuan Beef\\n- Stir-Fried Vegetables\\n- Peking Style Noodles\\n- Spring Rolls\\n- Vegetarian Dumplings\\n- Steamed Fish with Ginger and Spring Onion\\n- Spicy Salt & Pepper Squid\\n- Coconut Prawns'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_chain = SequentialChain(\n",
    "    chains = [name_chain, menu_items_chain], \n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', 'menu_items']\n",
    ")\n",
    "seq_chain({'cuisine': 'Chinese'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a066d-d759-4180-b8a8-787e7f86d303",
   "metadata": {},
   "source": [
    "And you can see, running the above gives us the cuisine, restaurant_name and menu_items as intended!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35642cd8-f121-45c0-89fb-861ff0773783",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Below, come up with another sequential chain with >= 3 nodes total, make it different than poems and food and show all output keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64dc4018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'character_type': 'Villain',\n",
       " 'character_name': '\\n\\nDr. Doom',\n",
       " 'story': \"\\n\\nVictor von Doom was born into a Romani family in Latveria. His parents were a powerful couple, with his father being a renowned alchemist and his mother a powerful sorceress. From a young age, Victor was exposed to both the science and the mysticism of his parents, and developed an interest in both.\\n\\nAt the age of 16, Victor was sent to the United States to attend college. While there, Victor excelled in his studies, particularly in the fields of physics and engineering. He also studied the mystic arts under a powerful sorcerer, and was able to combine his knowledge of science and magic to create powerful inventions. \\n\\nHowever, Victor's arrogance and ambition led him to attempt a dangerous experiment to contact the spirit realm. The experiment went horribly wrong, leaving Victor's face disfigured and his body scarred.\\n\\nFilled with rage and a desire for revenge, Victor returned to Latveria and used his newfound powers to take control of the country. He declared himself Doctor Doom, and set about creating a powerful empire. He used his knowledge of science and magic to create powerful weapons and inventions, and his iron-fisted rule kept the people of Latveria in check. \\n\\nDoctor Doom has\",\n",
       " 'weapon_names': ' a variety of powerful weapons at his disposal, including:\\n\\n1. Doomfire Gauntlet \\n2. Mystic Staff of Doom \\n3. Doomhammer \\n4. Doomblade \\n5. Doomstorm Cannon \\n6. Doomstrike Missile \\n7. Doomclaw \\n8. Doomfist Gauntlet \\n9. Doomcaster \\n10. Doombringer Axe'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_name = PromptTemplate(\n",
    "    input_variables = ['character_type'],\n",
    "    template = \"Suggest a suitable name for a {character_type}\"\n",
    ")\n",
    "\n",
    "# change to hmodel if you want to use huggingface\n",
    "cname_chain = LLMChain(llm=model, prompt=prompt_name, output_key=\"character_name\")\n",
    "\n",
    "prompt_items = PromptTemplate(\n",
    "    input_variables = ['character_name'],\n",
    "    template = \"Suggest a backstory for {character_name}\"\n",
    ")\n",
    "\n",
    "story_chain = LLMChain(llm=model, prompt=prompt_items, output_key=\"story\")\n",
    "\n",
    "prompt_name = PromptTemplate(\n",
    "    input_variables = ['story'],\n",
    "    template = \"Suggest names for a weapon that a character with the following backstory would use; {story}\"\n",
    ")\n",
    "\n",
    "# change to hmodel if you want to use huggingface\n",
    "weapon_chain = LLMChain(llm=model, prompt=prompt_name, output_key=\"weapon_names\")\n",
    "\n",
    "overall_chain = SequentialChain(chains=[cname_chain, story_chain, weapon_chain], input_variables=['character_type'], output_variables=['character_name', 'story', 'weapon_names'])\n",
    "overall_chain({'character_type': 'Villain'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
